
Lmod is automatically replacing "intel/2020.1.217" with "gcc/9.3.0".

DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME
DEBUG:hydra.core.utils:Setting JobRuntime:name=train
[2024-04-01 09:20:45,023][root][INFO] - project_dir: ${oc.env:WEBLINX_PROJECT_DIR}
seed: 123
project_name: llama_ft
data:
  num_proc: 8
  split_path: ${project_dir}/wl_data/splits.json
  base_dir: ${project_dir}/wl_data/demonstrations/
train:
  split: train
  num_epochs: 3
  learning_rate: 5.0e-05
  batch_size_per_device: 4
  gradient_accumulation_steps: 4
  dataloader_num_workers: 8
  gradient_checkpointing: true
  use_accelerator_device_map: false
  use_auto_device_map: true
  warmup_ratio: 0
  scheduler: linear
  optim: adamw_torch
eval:
  split: valid
  batch_size_per_device: 8
  result_dir: ${project_dir}/results/${project_name}/${eval.split}/${model.name}
  load_from_save_dir: true
model:
  name: princeton-nlp/Sheared-LLaMA-1.3B
  tokenizer: ${model.name}
  template_tokenizer: ${model.tokenizer}
  max_inp_len: null
  max_out_len: 256
  use_rope: true
  use_flash_attention_2: false
  save_dir: ${project_dir}/checkpoints/${project_name}/${model.name}
candidates:
  k: 10
  model: McGill-NLP/MiniLM-L6-dmr
  project_name: dmr
  split: ${eval.split}
  train_path: ${project_dir}/wl_data/candidates/train.jsonl
  path: ${project_dir}/wl_data/candidates/${candidates.split}.jsonl

Loading candidates: 0it [00:00, ?it/s]Loading candidates: 3it [00:00, 7161.59it/s]
[2024-04-01 09:21:15,385][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Processing demos into input records:   0%|          | 0/1 [00:00<?, ?it/s]Processing demos into input records: 100%|██████████| 1/1 [00:00<00:00, 16.14it/s]
Building input records:   0%|          | 0/25 [00:00<?, ?it/s]Building input records:   4%|▍         | 1/25 [00:00<00:08,  2.76it/s]Building input records:   8%|▊         | 2/25 [00:00<00:09,  2.49it/s]Building input records:  12%|█▏        | 3/25 [00:01<00:08,  2.70it/s]Building input records:  16%|█▌        | 4/25 [00:01<00:06,  3.28it/s]Building input records:  20%|██        | 5/25 [00:01<00:08,  2.41it/s]Building input records:  28%|██▊       | 7/25 [00:02<00:05,  3.56it/s]Building input records:  32%|███▏      | 8/25 [00:02<00:04,  3.60it/s]Building input records:  36%|███▌      | 9/25 [00:02<00:04,  3.53it/s]Building input records:  40%|████      | 10/25 [00:03<00:03,  3.79it/s]Building input records:  44%|████▍     | 11/25 [00:03<00:03,  4.18it/s]Building input records:  48%|████▊     | 12/25 [00:03<00:03,  4.23it/s]Building input records:  52%|█████▏    | 13/25 [00:03<00:02,  4.33it/s]Building input records:  56%|█████▌    | 14/25 [00:04<00:03,  3.65it/s]Building input records:  60%|██████    | 15/25 [00:04<00:02,  4.41it/s]Building input records:  64%|██████▍   | 16/25 [00:04<00:01,  5.15it/s]Building input records:  68%|██████▊   | 17/25 [00:04<00:01,  5.04it/s]Building input records:  72%|███████▏  | 18/25 [00:04<00:01,  5.69it/s]Building input records:  80%|████████  | 20/25 [00:04<00:00,  6.04it/s]Building input records:  84%|████████▍ | 21/25 [00:05<00:00,  5.54it/s]Building input records:  92%|█████████▏| 23/25 [00:05<00:00,  7.01it/s]Building input records: 100%|██████████| 25/25 [00:05<00:00,  4.67it/s]

No chat template is defined for this tokenizer - using the default template for the LlamaTokenizerFast class. If the default is not appropriate for your model, please set `tokenizer.chat_template` to an appropriate template. See https://huggingface.co/docs/transformers/main/chat_templating for more information.

Error executing job with overrides: ['+variant=ft_1.3b']
Traceback (most recent call last):
  File "/scratch/kapmcgil/weblinx/modeling/llama/train.py", line 96, in main
    training_args = TrainingArguments(
  File "<string>", line 117, in __init__
  File "/home/kapmcgil/.local/lib/python3.10/site-packages/transformers/training_args.py", line 1387, in __post_init__
    raise ValueError(
ValueError: Your setup doesn't support bf16/gpu. You need torch>=1.10, using Ampere GPU with cuda>=11.0

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
