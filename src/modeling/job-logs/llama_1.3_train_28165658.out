
Lmod is automatically replacing "intel/2020.1.217" with "gcc/9.3.0".

DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME
DEBUG:hydra.core.utils:Setting JobRuntime:name=train
[2024-04-01 09:40:46,602][root][INFO] - project_dir: ${oc.env:WEBLINX_PROJECT_DIR}
seed: 123
project_name: llama_ft
data:
  num_proc: 8
  split_path: ${project_dir}/wl_data/splits.json
  base_dir: ${project_dir}/wl_data/demonstrations/
train:
  split: train
  num_epochs: 3
  learning_rate: 5.0e-05
  batch_size_per_device: 4
  gradient_accumulation_steps: 4
  dataloader_num_workers: 8
  gradient_checkpointing: true
  use_accelerator_device_map: false
  use_auto_device_map: true
  warmup_ratio: 0
  scheduler: linear
  optim: adamw_torch
eval:
  split: valid
  batch_size_per_device: 8
  result_dir: ${project_dir}/results/${project_name}/${eval.split}/${model.name}
  load_from_save_dir: true
model:
  name: princeton-nlp/Sheared-LLaMA-1.3B
  tokenizer: ${model.name}
  template_tokenizer: ${model.tokenizer}
  max_inp_len: null
  max_out_len: 256
  use_rope: true
  use_flash_attention_2: false
  save_dir: ${project_dir}/checkpoints/${project_name}/${model.name}
candidates:
  k: 10
  model: McGill-NLP/MiniLM-L6-dmr
  project_name: dmr
  split: ${eval.split}
  train_path: ${project_dir}/wl_data/candidates/train.jsonl
  path: ${project_dir}/wl_data/candidates/${candidates.split}.jsonl

Loading candidates: 0it [00:00, ?it/s]Loading candidates: 1it [00:00,  6.27it/s]Loading candidates: 3it [00:00, 18.76it/s]
[2024-04-01 09:41:17,843][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Processing demos into input records:   0%|          | 0/1 [00:00<?, ?it/s]Processing demos into input records: 100%|██████████| 1/1 [00:00<00:00, 14.09it/s]
Building input records:   0%|          | 0/25 [00:00<?, ?it/s]Building input records:   4%|▍         | 1/25 [00:00<00:22,  1.05it/s]Building input records:   8%|▊         | 2/25 [00:01<00:11,  2.02it/s]Building input records:  12%|█▏        | 3/25 [00:01<00:07,  2.90it/s]Building input records:  16%|█▌        | 4/25 [00:01<00:06,  3.25it/s]Building input records:  20%|██        | 5/25 [00:02<00:12,  1.61it/s]Building input records:  28%|██▊       | 7/25 [00:02<00:06,  2.75it/s]Building input records:  32%|███▏      | 8/25 [00:03<00:05,  3.18it/s]Building input records:  36%|███▌      | 9/25 [00:03<00:04,  3.64it/s]Building input records:  40%|████      | 10/25 [00:03<00:05,  2.81it/s]Building input records:  44%|████▍     | 11/25 [00:04<00:04,  3.37it/s]Building input records:  48%|████▊     | 12/25 [00:04<00:03,  3.37it/s]Building input records:  52%|█████▏    | 13/25 [00:04<00:03,  3.59it/s]Building input records:  56%|█████▌    | 14/25 [00:07<00:12,  1.13s/it]Building input records:  60%|██████    | 15/25 [00:08<00:08,  1.12it/s]Building input records:  64%|██████▍   | 16/25 [00:08<00:05,  1.52it/s]Building input records:  68%|██████▊   | 17/25 [00:08<00:04,  1.98it/s]Building input records:  72%|███████▏  | 18/25 [00:08<00:02,  2.59it/s]Building input records:  80%|████████  | 20/25 [00:08<00:01,  3.77it/s]Building input records:  84%|████████▍ | 21/25 [00:08<00:00,  4.02it/s]Building input records:  92%|█████████▏| 23/25 [00:08<00:00,  5.64it/s]Building input records: 100%|██████████| 25/25 [00:09<00:00,  2.77it/s]

No chat template is defined for this tokenizer - using the default template for the LlamaTokenizerFast class. If the default is not appropriate for your model, please set `tokenizer.chat_template` to an appropriate template. See https://huggingface.co/docs/transformers/main/chat_templating for more information.

Map:   0%|          | 0/25 [00:00<?, ? examples/s]Map: 100%|██████████| 25/25 [00:00<00:00, 808.18 examples/s]
/home/kapmcgil/.local/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)
  warnings.warn(
[2024-04-01 09:41:36,918][accelerate.utils.other][WARNING] - Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/3 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
 33%|███▎      | 1/3 [00:27<00:55, 27.77s/it] 67%|██████▋   | 2/3 [00:47<00:22, 22.94s/it]100%|██████████| 3/3 [01:14<00:00, 24.79s/it]                                             100%|██████████| 3/3 [01:14<00:00, 24.79s/it]100%|██████████| 3/3 [01:14<00:00, 24.82s/it]
{'train_runtime': 74.4085, 'train_samples_per_second': 1.008, 'train_steps_per_second': 0.04, 'train_loss': 1.575461705525716, 'epoch': 1.71}
